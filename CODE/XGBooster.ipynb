{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8d09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4346f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\sukha\\Downloads\\DATA_SCIENCE\\DATASET\\Cleaned_DATA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac6c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Drop Columns ---\n",
    "# Drop 'Country' (constant) and 'Month' (high missing values)\n",
    "df = df.drop(columns=['Country', 'Month'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e748901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Define Features (X) and Target (y) ---\n",
    "target_column = 'Reservoir_Water_Storage_BCM'\n",
    "y = df[target_column]\n",
    "X = df.drop(columns=[target_column])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e73d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Handle Categorical Features (One-Hot Encoding) ---\n",
    "categorical_cols = ['State', 'District', 'Reservoir Basin Name', 'Reservoir Name']\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "feature_names = X.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b16fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important for XGBoost: Sanitize column names\n",
    "# XGBoost can have issues with special characters like ':', ',', '[', ']', and '<' in feature names.\n",
    "# Replacing these characters with underscores is a robust practice.\n",
    "X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef9b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Complete. X_train shape: (42900, 584)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Split Data ---\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Data Preparation Complete. X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c4b83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Complete. X_train shape: (42900, 584)\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”¥ FIX: Convert Pandas DataFrames/Series to NumPy arrays for XGBoost compatibility\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "print(f\"Data Preparation Complete. X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ba1569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost Regressor Model Training...\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialize Model ---\n",
    "# Initialize the XGBoost Regressor\n",
    "# A common starting configuration:\n",
    "# n_estimators: Number of boosting rounds (trees)\n",
    "# learning_rate: Controls the step size shrinkage (lower is generally safer)\n",
    "# objective: Specifies the type of loss function (reg:squarederror is standard for regression)\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror', \n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    random_state=42, \n",
    "    n_jobs=-1 # Use all available cores\n",
    ")\n",
    "\n",
    "print(\"Starting XGBoost Regressor Model Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96060c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Regressor Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Train Model ---\n",
    "# Train the model using the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"XGBoost Regressor Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "461a4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Make Predictions ---\n",
    "# Predict the reservoir water storage on the test set\n",
    "y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658bf765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Calculate Evaluation Metrics ---\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfbd5893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- XGBoost Regressor Model Evaluation ---\n",
      "Mean Absolute Error (MAE): 0.0538\n",
      "Mean Squared Error (MSE): 0.0270\n",
      "Root Mean Squared Error (RMSE): 0.1644\n",
      "R-squared (R^2): 0.9334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Display Results ---\n",
    "print(\"--- XGBoost Regressor Model Evaluation ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R-squared (R^2): {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3894c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Residual: -0.0012\n",
      "Standard Deviation of Residuals: 0.1644\n"
     ]
    }
   ],
   "source": [
    "# Optional: Examine residuals\n",
    "# The difference between actual and predicted values. Ideally, these should be close to zero.\n",
    "residuals = y_test - y_pred\n",
    "print(f\"\\nMean Residual: {residuals.mean():.4f}\")\n",
    "print(f\"Standard Deviation of Residuals: {residuals.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41952b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "with open('models/xgboost.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c041c07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovered feature names from original dataset.\n",
      "Features from X_train:\n",
      "Categorical: ['Country', 'State', 'District', 'Month', 'Reservoir Basin Name', 'Reservoir Name']\n",
      "Numeric: ['Year', 'Full_Reservoir_Capacity_BCM', 'Reservoir_Water_Level_M', 'Reservoir_Water_Storage_BCM']\n",
      "âœ“ XGBoost model saved\n",
      "\n",
      "âœ“ All models saved with proper feature information!\n",
      "\n",
      "Feature summary:\n",
      "  - 6 categorical features\n",
      "  - 4 numeric features\n",
      "  - 595 total features after encoding\n"
     ]
    }
   ],
   "source": [
    "# Run this code in each of your model notebooks to properly save models\n",
    "# Replace the variables with your actual variable names\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# ====================\n",
    "# IDENTIFY YOUR FEATURES\n",
    "# ====================\n",
    "\n",
    "# Option 1: If you still have X_train available\n",
    "if 'X_train' in locals():\n",
    "    if 'X_train' in locals():\n",
    "    # If X_train is a DataFrame\n",
    "        if hasattr(X_train, 'columns'):\n",
    "            feature_names = X_train.columns.tolist()\n",
    "            categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "            numeric_columns = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        else:\n",
    "            # If X_train is a numpy array, use the original dataset to recover feature names\n",
    "            data = pd.read_csv(r\"C:\\Users\\sukha\\Downloads\\DATA_SCIENCE\\DATASET\\Cleaned_DATA.csv\")\n",
    "            target_column = 'Production'  # ðŸ”¹ Change this to your target\n",
    "            feature_columns = [col for col in data.columns if col != target_column]\n",
    "            \n",
    "            # Handle encoding just like during training\n",
    "            X_encoded = pd.get_dummies(data[feature_columns], drop_first=True)\n",
    "            feature_names = X_encoded.columns.tolist()\n",
    "    \n",
    "            categorical_columns = data[feature_columns].select_dtypes(include=['object']).columns.tolist()\n",
    "            numeric_columns = data[feature_columns].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "            print(\"Recovered feature names from original dataset.\")\n",
    "\n",
    "    \n",
    "    print(\"Features from X_train:\")\n",
    "    print(f\"Categorical: {categorical_columns}\")\n",
    "    print(f\"Numeric: {numeric_columns}\")\n",
    "\n",
    "# Option 2: If X_train is not available, get from original data\n",
    "else:\n",
    "    # Load your original data\n",
    "    data = pd.read_csv(r\"C:\\Users\\sukha\\Downloads\\DATA_SCIENCE\\DATASET\\Cleaned_DATA.csv\")\n",
    "    \n",
    "    # Identify your target column (adjust the name!)\n",
    "    target_column = 'Production'  # CHANGE THIS TO YOUR ACTUAL TARGET\n",
    "    \n",
    "    # Get feature columns (everything except target)\n",
    "    feature_columns = [col for col in data.columns if col != target_column]\n",
    "    \n",
    "    categorical_columns = data[feature_columns].select_dtypes(include=['object']).columns.tolist()\n",
    "    numeric_columns = data[feature_columns].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(\"Features from original data:\")\n",
    "    print(f\"Categorical: {categorical_columns}\")\n",
    "    print(f\"Numeric: {numeric_columns}\")\n",
    "    \n",
    "    # If you used one-hot encoding, get those feature names\n",
    "    X_encoded = pd.get_dummies(data[feature_columns], drop_first=True)\n",
    "    feature_names = X_encoded.columns.tolist()\n",
    "\n",
    "# ====================\n",
    "# SAVE YOUR MODELS\n",
    "# ====================\n",
    "\n",
    "# FOR LINEAR REGRESSION\n",
    "if 'linear_model' in locals() or 'model' in locals():  # Adjust variable name\n",
    "    model_to_save = linear_model if 'linear_model' in locals() else model\n",
    "    \n",
    "    model_data = {\n",
    "        'model': model_to_save,\n",
    "        'feature_names': feature_names,\n",
    "        'categorical_columns': categorical_columns,\n",
    "        'numeric_columns': numeric_columns\n",
    "    }\n",
    "    \n",
    "    with open('models/linear_regression.pkl', 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    print(\"âœ“ Linear Regression model saved\")\n",
    "\n",
    "# FOR RANDOM FOREST\n",
    "if 'random_forest_model' in locals():\n",
    "    model_data = {\n",
    "        'model': random_forest_model,\n",
    "        'feature_names': feature_names,\n",
    "        'categorical_columns': categorical_columns,\n",
    "        'numeric_columns': numeric_columns\n",
    "    }\n",
    "    \n",
    "    with open('models/random_forest.pkl', 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    print(\"âœ“ Random Forest model saved\")\n",
    "\n",
    "# FOR XGBOOST\n",
    "if 'xgb_model' in locals():\n",
    "    model_data = {\n",
    "        'model': xgb_model,\n",
    "        'feature_names': feature_names,\n",
    "        'categorical_columns': categorical_columns,\n",
    "        'numeric_columns': numeric_columns\n",
    "    }\n",
    "    \n",
    "    with open('models/xgboost.pkl', 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    print(\"âœ“ XGBoost model saved\")\n",
    "\n",
    "# FOR KNN\n",
    "if 'knn_model' in locals():\n",
    "    model_data = {\n",
    "        'model': knn_model,\n",
    "        'feature_names': feature_names,\n",
    "        'categorical_columns': categorical_columns,\n",
    "        'numeric_columns': numeric_columns\n",
    "    }\n",
    "    \n",
    "    with open('models/knn.pkl', 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    print(\"âœ“ KNN model saved\")\n",
    "\n",
    "# FOR NEURAL NETWORK\n",
    "if 'model' in locals():\n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'feature_names': feature_names,\n",
    "        'categorical_columns': categorical_columns,\n",
    "        'numeric_columns': numeric_columns\n",
    "    }\n",
    "    \n",
    "    with open('models/neural_network.pkl', 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    print(\"âœ“ Neural Network model saved\")\n",
    "\n",
    "# ====================\n",
    "# SAVE FEATURE INFO\n",
    "# ====================\n",
    "\n",
    "# Get unique values for categorical columns\n",
    "data = pd.read_csv(r\"C:\\Users\\sukha\\Downloads\\DATA_SCIENCE\\DATASET\\Cleaned_DATA.csv\")\n",
    "categorical_values = {}\n",
    "for col in categorical_columns:\n",
    "    if col in data.columns:\n",
    "        categorical_values[col] = data[col].dropna().unique().tolist()\n",
    "\n",
    "feature_info = {\n",
    "    'categorical_features': categorical_columns,\n",
    "    'numeric_features': numeric_columns,\n",
    "    'all_features': feature_names,\n",
    "    'categorical_values': categorical_values\n",
    "}\n",
    "\n",
    "with open('models/feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ“ All models saved with proper feature information!\")\n",
    "print(f\"\\nFeature summary:\")\n",
    "print(f\"  - {len(categorical_columns)} categorical features\")\n",
    "print(f\"  - {len(numeric_columns)} numeric features\")\n",
    "print(f\"  - {len(feature_names)} total features after encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9a6dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count after fix: 584\n",
      "âœ… Model saved successfully with corrected feature mapping.\n",
      "âœ… Feature info saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Ensure you have your dataset\n",
    "# Load original data\n",
    "data = pd.read_csv(r\"C:\\\\Users\\\\sukha\\\\Downloads\\\\DATA_SCIENCE\\\\DATASET\\\\Cleaned_DATA.csv\")\n",
    "\n",
    "# --- FIX: Drop the same columns as in training ---\n",
    "data = data.drop(columns=['Country', 'Month'])\n",
    "\n",
    "target_column = 'Reservoir_Water_Storage_BCM'  # change this if different\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Perform encoding just like during training\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Important: Sanitize column names for feature list consistency\n",
    "X_encoded.columns = X_encoded.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
    "\n",
    "feature_names = X_encoded.columns.tolist()\n",
    "# NOTE: The feature count should now be 584\n",
    "print(f\"Feature count after fix: {len(feature_names)}\")\n",
    "\n",
    "# Identify feature types for feature_info.json\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_columns = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Save model + feature info\n",
    "model_data = {\n",
    "    \"model\": xgb_model,  # your trained XGBRegressor\n",
    "    \"feature_names\": feature_names,\n",
    "    \"categorical_columns\": categorical_columns,\n",
    "    \"numeric_columns\": numeric_columns\n",
    "}\n",
    "\n",
    "with open('models/xgboost.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"âœ… Model saved successfully with corrected feature mapping.\")\n",
    "\n",
    "# Save categorical value info\n",
    "# Load original data again to get full categorical values before dropping 'Country', 'Month' (optional, but robust)\n",
    "original_data = pd.read_csv(r\"C:\\\\Users\\\\sukha\\\\Downloads\\\\DATA_SCIENCE\\\\DATASET\\\\Cleaned_DATA.csv\")\n",
    "\n",
    "categorical_values = {\n",
    "    col: original_data[col].dropna().unique().tolist()\n",
    "    for col in categorical_columns if col in original_data.columns\n",
    "}\n",
    "\n",
    "feature_info = {\n",
    "    \"categorical_features\": categorical_columns,\n",
    "    \"numeric_features\": numeric_columns,\n",
    "    \"all_features\": feature_names,\n",
    "    \"categorical_values\": categorical_values\n",
    "}\n",
    "\n",
    "with open('models/feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "print(\"âœ… Feature info saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
